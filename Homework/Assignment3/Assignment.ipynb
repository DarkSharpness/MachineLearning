{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DC2E6D77D61947D98468860C12ABEB2F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 支持向量机（Support Vector Machine）\n",
    "\n",
    "## 简介\n",
    "\n",
    "支持向量机的原始算法于1963年由苏联科学家 **弗拉基米尔·万普尼克(Владимир Наумович Вапник)** 等发明，最初发表于俄文期刊 **自动化与远程控制 (АВТОМАТИКА И ТЕЛЕМЕХАНИКА)**. \n",
    "\n",
    "点击 [Узнавание образов при помощи обобщенных портретов](http://www.mathnet.ru/links/15c09987c4667b628d040141610ef1b6/at11885.pdf) 查看原始俄语论文，也可以查看英语翻译版 [Recognition of Patterns with help of Generalized Portraits](http://web.cs.iastate.edu/~cs573x/vapnik-portraits1963.pdf)(这个链接可能打不开了)\n",
    "\n",
    "直到1990年代，由于核技巧(kernel trick)的引入，使得SVM有了非线性的分类能力，SVM才开始获得成功，并引起了一次人工智能的高潮。\n",
    "\n",
    "在这次作业中，你们需要实现:\n",
    "- 最基本的线性支持向量机\n",
    "- 梯度下降的优化算法\n",
    "- 序列最小优化(SMO)算法和核技巧\n",
    "- 学会使用scikit-learn中的SVM\n",
    "\n",
    "并回答一些相关问题\n",
    "\n",
    "**本次小作业截止时间：5.7, 请在canvas系统中及时提交。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "836CE24CB23B47259BB5AF90C2B5D437",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 线性 SVM 分类器与梯度下降\n",
    "\n",
    "在这一部分中，你们需要实现最基本的线性SVM分类器，并完成一个二分类问题，首先我们需要制造一点数据。运行下面的代码就可以得到一个简单的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "F42D6DCD46B3414A9BAA4EEFAD7EA806",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1024)\n",
    "\n",
    "def data_visualization(x, y, pdim=2):\n",
    "    category = {'+1': [], '-1': []}\n",
    "    for point, label in zip(x, y):\n",
    "        if label == 1.0: category['+1'].append(point)\n",
    "        else: category['-1'].append(point)\n",
    "    fig = plt.figure()\n",
    "    if pdim == 2 or pdim == 1:\n",
    "        ax = fig.add_subplot(111)\n",
    "    elif pdim == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for label, pts in category.items():\n",
    "        pts = np.array(pts)\n",
    "        if pdim == 1:\n",
    "            ax.scatter(pts[:, 0], label=label)\n",
    "        elif pdim == 2:\n",
    "            ax.scatter(pts[:, 0], pts[:, 1], label=label)\n",
    "        elif pdim == 3:\n",
    "            if label == '+1':\n",
    "                c = 'blue'\n",
    "                m = 'o'\n",
    "            else:\n",
    "                c = 'black'\n",
    "                m = 'x'\n",
    "            ax.scatter(pts[:, 0], pts[:, 1], pts[:, 2], c=c, marker=m)\n",
    "    plt.show() \n",
    "\n",
    "\n",
    "# random a dataset on 2D plane\n",
    "def simple_synthetic_data(n, n0=5, n1=5): # n: number of points, n0 & n1: number of points on boundary\n",
    "    # random a line on the plane\n",
    "    w = np.random.rand(2) \n",
    "    w = w / np.sqrt(w.dot(w))\n",
    "    \n",
    "    # random n points \n",
    "    x = np.random.rand(n, 2) * 2 - 1\n",
    "    d = (np.random.rand(n) + 1) * np.random.choice([-1,1],n,replace=True) # random distance from point to the decision line, d in [-2,-1] or [1,2]. d=-1 or d=1 indicate the boundary in svm\n",
    "    d[:n0] = -1\n",
    "    d[n0:n0+n1] = 1\n",
    "    \n",
    "    # shift x[i] to make the distance between x[i] and the decision become d[i]\n",
    "    x = x - x.dot(w).reshape(-1,1) * w.reshape(1,2) + d.reshape(-1,1) * w.reshape(1,2)\n",
    "    \n",
    "    # create labels\n",
    "    y = np.zeros(n)\n",
    "    y[d < 0] = -1\n",
    "    y[d >= 0] = 1\n",
    "    return x, y\n",
    "\n",
    "x_data, y_data = simple_synthetic_data(200)\n",
    "data_visualization(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9A17F0311664EC895949B8F9BD2B80E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 线性SVM\n",
    "\n",
    "对于上文生成的这组二分类数据，在课堂上，我们首先引入间隔函数: \n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}^{(i)}=y^{(i)}(\\boldsymbol{w^\\top}\\boldsymbol{x}^{(i)} + b)\n",
    "$$\n",
    "\n",
    "其中，$\\boldsymbol{w}$为分界(超)平面的一个法向量，$\\boldsymbol{x}^{(i)}$ 为第$i$个样本，$\\boldsymbol{y}^{(i)}$ 为第$i$个样本的标签，我们很容易看出$\\boldsymbol{w^\\top}\\boldsymbol{x}^{(i)} + b$类似于一个 **没有取绝对值的点到平面的距离公式** ，$\\boldsymbol{y}^{(i)}$则指示了在分界平面之上或者之下，即样本点的类别。我们希望**所有点到分类边界的距离越远越好**，此外，在运算中，将法向量归一化是一个很合理的想法，于是，问题转化为\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " & \\max_{\\gamma,\\boldsymbol{w},b} \\gamma \\\\\\\\\n",
    "\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge {\\gamma},\\quad i=1..,m\\\\\n",
    " & \\|\\boldsymbol{w}\\|_2 = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\gamma = \\min\\{\\hat{\\gamma}^{(1)}, \\hat{\\gamma}^{(1)}, \\hat{\\gamma}^{(1)}, \\cdots, \\hat{\\gamma}^{(n)}\\}$，这个问题等价于\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " & \\max_{\\gamma,\\boldsymbol{w},b} \\frac{\\gamma}{\\|\\boldsymbol{w}\\|_2} \\\\\\\\\n",
    "\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge {\\gamma},\\quad i=1..,m\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们可以规定数据点最小的距离为1，则\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " & \\max_{\\boldsymbol{w},b} \\frac{1}{\\|\\boldsymbol{w}\\|_2} \\equiv \\min_{\\boldsymbol{w},b} \\frac{1}{2}\\|\\boldsymbol{w}\\|_2^2\\\\\\\\\n",
    "\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge 1,\\quad i=1..,m\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "这样一来，我们就把这个问题从一个关于$\\gamma,w,b$非凸优化转化为一个关于$w, b$的凸优化。\n",
    "\n",
    "所以我们可以通过之前讲过的梯度下降方法来求解这个问题，即优化一个损失函数。我们在课上学习了Hinge Loss，对于一组数据，可以写成\n",
    "\n",
    "$$\n",
    "{\\cal{L}}_{\\mathrm{SVM}} = C\\|\\boldsymbol{w}\\|_2^2 + \\sum_{i=1}^{N}\\max(0, 1 - y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b))\n",
    "$$\n",
    "\n",
    "其中 $C$ 为可调节的常数，作为 L2 正则化系数\n",
    "\n",
    "下面请回答这样几个问题，请使用markdown语法直接在题目下方作答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85A7EF9EE0D145188AA65C1F57219778",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Question1：\n",
    "\n",
    "为什么最小化Hinge Loss等价于优化上面关于$\\boldsymbol{w},b$的优化问题？（提示：不必严格证明，只需简单合理的说明）\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "暂时忽略正则化项 $C\\|\\boldsymbol{w}\\|_2^2$，\n",
    "\n",
    "对于正确的样本 $y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge 1$，在 Hinge Loss 中对应项数 $\\max(0, 1- y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b)) = 0$. 同理, 错误的样本, 这一项大于0.\n",
    "\n",
    "在线性可分的情况下, 最优解是使得所有的样本都满足 $y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge 1$，Hinge Loss 取到最小值 $0$, 可以认为是最优解。因此, 最小化 Hinge Loss 等价于优化上面关于 $\\boldsymbol{w}, b$ 的优化问题。\n",
    "\n",
    "#### Question2：\n",
    "\n",
    "证明 $f(\\boldsymbol{w}, b) = C\\|\\boldsymbol{w}\\|_2^2 + \\max(0, 1 - y(\\boldsymbol{w}^\\top \\boldsymbol{x} + b))$ 是凸函数（提示：请复习凸函数的定义，你可以利用任何在先修课程中学过的性质、方法和技巧来证明，比如可以通过函数复合的性质来证明，合理即可）\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "凸函数之和仍然是凸函数，凸函数的复合函数仍然是凸函数。因此，我们只需要证明 $C\\|\\boldsymbol{w}\\|_2^2$ 和 $\\max(0, 1 - y(\\boldsymbol{w}^\\top \\boldsymbol{x} + b))$ 是凸函数即可。\n",
    "\n",
    "前者显然, 是一个二次函数，是凸函数。只需要证明后者是凸函数。\n",
    "\n",
    "注意到两个凸函数逐点取 $\\max$ 仍然是凸函数。因此，只需要证明 $1 - y(\\boldsymbol{w}^\\top \\boldsymbol{x} + b)$ 是凸函数。而这个函数是一个仿射函数，是凸函数。\n",
    "\n",
    "#### Question3：\n",
    "\n",
    "对于一个凸函数 $f(\\boldsymbol{x})$，假设我们找到了一个局部最优解 $\\boldsymbol{x_0}$，证明这个最优解是全局最优解。（提示：凸函数定义）\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "凸函数局部最优一定是全局最优.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F281792FE34441858214263B6C227A2D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果你成功回答了上面的问题，你就可以理解我们可以通过梯度下降的方式找到SVM的最优解，下面就需要实现基本的SVM以及梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "037C9A6795484CB68A4E27BCEAFFD019",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svm_sgd(w, b, X_in, y_in, reg):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D, we operate on minibatches of N examples.\n",
    "    - w: The weight, a numpy array of shape (D,) containing weights \n",
    "    - b: The bias, a constant \n",
    "    - X_in: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y_in: A numpy array of shape (N,) containing training labels; y[i] = -1/ 1 means that X[i] has label -1/1\n",
    "    - reg: regularization factor, constant, float\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss, loss as single float\n",
    "    - dw, gradient with respect to weights W; an array of same shape as W\n",
    "    - db, gradient with respect to bias b, a constant\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    dw = np.zeros(w.shape)\n",
    "    db = 0.0\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement SVM loss, storing the result in loss.                           #\n",
    "    # **No explicit loop allowed, otherwise your points will be deducted**      #\n",
    "    #############################################################################\n",
    "    \n",
    "    part = np.maximum(0, 1 - y_in * (X_in @ w + b))\n",
    "    loss = reg * w.dot(w) + part.sum()\n",
    "    # print(\"Sum:\", part.sum())\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement SVM gradient respect to w and b,                                #\n",
    "    # storing the result in dw, db, respectively.                               #\n",
    "    # **No explicit loop allowed, otherwise your points will be deducted**      #\n",
    "    # **Always assume that the loss function is differentiable**                #\n",
    "    #############################################################################\n",
    "        \n",
    "    # Shape of w is (D,)\n",
    "    # Shape of part,y is (N,)\n",
    "    # Shape of x is (N,D)\n",
    "    # Use part to determine \n",
    "\n",
    "    tmp = np.where(part > 0, -y_in, 0)\n",
    "    dw = 2 * reg * w + tmp @ X_in\n",
    "    db = tmp.sum()\n",
    "    # print(\"DEBUG\")\n",
    "    # print(loss.shape)\n",
    "    # print(dw.shape)\n",
    "    # print(db.shape)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return loss, dw, db\n",
    "\n",
    "def demo():\n",
    "    x = np.array([1,2,3])\n",
    "    y = np.array([1,2])\n",
    "    X = np.matrix([[1,2],[2,3],[3,4]])\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(X.shape)\n",
    "    print('fxxk')\n",
    "    print(y.reshape(-1,1).shape)\n",
    "    print(x.reshape(1,-1).shape)\n",
    "    print('fxxk')\n",
    "    print(x * X)\n",
    "    print(X @ y)\n",
    "    print(x * x)\n",
    "\n",
    "# demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93BF47C3F42B47ACB1046C507964BBB3",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "到这里你们已经完成了基本的线性SVM，及其梯度计算，下面的一些代码可以用于检查调试你的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8A52B96678B149E98021C2DD4564635C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange, seed\n",
    "\n",
    "seed(1024)\n",
    "def check_gradient_simple():\n",
    "    \"\"\"\n",
    "    This function check gradient using numerical method, formally:\n",
    "    \\frac{\\partial f}{\\partial w_i} = (f(w_i + h) - f(w_i - h)) / 2h\n",
    "    The relative error should be very small\n",
    "    \"\"\"\n",
    "    reg = 0.5\n",
    "    for i in range(10):\n",
    "        w = np.random.randn(2) \n",
    "        b = np.random.randn(1)\n",
    "        loss, dw, db = svm_sgd(w, b, x_data, y_data, reg)\n",
    "        h = 1e-5\n",
    "        idx = randrange(2)\n",
    "\n",
    "        wp = w.copy()\n",
    "        wp[idx] += h\n",
    "\n",
    "        wm = w.copy()\n",
    "        wm[idx] -= h\n",
    "\n",
    "        evalwp, _, _ = svm_sgd(wp, b, x_data, y_data, reg)\n",
    "        evalwm, _, _ = svm_sgd(wm, b, x_data, y_data, reg)\n",
    "\n",
    "        grad_num_w = (evalwp - evalwm) / (2 * h)\n",
    "\n",
    "        bp = b + h\n",
    "        bm = b - h\n",
    "        evalbp, _, _ = svm_sgd(w, bp, x_data, y_data, reg)\n",
    "        evalbm, _, _ = svm_sgd(w, bm, x_data, y_data, reg)\n",
    "        grad_num_b = (evalbp - evalbm) / (2 * h)\n",
    "\n",
    "        rel_dw_error = np.abs(dw[idx] - grad_num_w) / (np.abs(dw[idx]) + np.abs(grad_num_w))\n",
    "        rel_db_error = np.abs(db - grad_num_b) / (np.abs(db) + np.abs(grad_num_b))\n",
    "\n",
    "        print('#%d randomly calculate svm loss: %f, relative dw error: %.12f, relative db error: %.12f'\n",
    "              % (i, loss, rel_dw_error, rel_db_error))\n",
    "\n",
    "check_gradient_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8CE2B66F63A47539A7AB98F62A7BE03",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果你的相对误差大多数量级在 $10^{-10}$ 及以下，说明你通过了梯度检查，说明你的代码实现正确。然后请你回答以下问题：\n",
    "\n",
    "#### Question4：\n",
    "\n",
    "在计算梯度时，我们假设函数可微，但事实上并非如此，如果你运行梯度检查足够多次，你会发现梯度检查可能失败，请说明在何种情况下我们无法计算梯度（简要说明，无需严格证明，合理即可）\n",
    "\n",
    "#### Answer：\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "回答问题之后，就可以开始训练简单的线性SVM分类器了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "E0CB2CD76C384009A9A539A4707C3306",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_svm_sgd(w, b, num_epoch, lr=5e-3, reg=0.0):\n",
    "    \"\"\"\n",
    "    Implement SVM SGD training algorithm here, we randomly initilaize the weight and bias\n",
    "    \"\"\"\n",
    "    \n",
    "    for e in range(num_epoch):\n",
    "        loss, dw, db = svm_sgd(w, b, x_data, y_data, reg)\n",
    "        print('epoch %d, the loss is %f' % (e+1, loss))\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement Gradient Descent with learning decay to train SVM               #\n",
    "    #############################################################################\n",
    "        lr_e = lr\n",
    "        w = w - lr_e * dw\n",
    "        b = b - lr_e * db\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return w, b\n",
    "\n",
    "w = np.random.randn(2)\n",
    "b = np.random.randn(1)\n",
    "w, b = train_svm_sgd(w, b, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "372EC50050AA407B8C57647A88AEC04E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果一切正常，那么你可以看到损失函数降到了接近0,这是显然的，因为数据是线性可分的，我们可以将训练得到的结果可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5ECAA2857EFF45B88956595DABB51F53",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "category = {'+1': [], '-1': []}\n",
    "for point, label in zip(x_data, y_data):\n",
    "    if label == 1.0: category['+1'].append(point)\n",
    "    else: category['-1'].append(point)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot points\n",
    "for label, pts in category.items():\n",
    "    pts = np.array(pts)\n",
    "    ax.scatter(pts[:, 0], pts[:, 1], label=label)\n",
    "\n",
    "# calculate weight\n",
    "weight = w\n",
    "bias = b\n",
    "# plot the model: wx+b\n",
    "x1 = np.min(x_data[:, 0])\n",
    "y1 = (-bias - weight[0] * x1) / weight[1]\n",
    "x2 = np.max(x_data[:, 0])\n",
    "y2 = (-bias - weight[0] * x2) / weight[1]\n",
    "ax.plot([x1, x2], [y1, y2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10CC2C73B1054365B0118207C2E6B50C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "通过可视化，可以看到我们已经找到了一个分类边界。如果你多次运行上面的训练和可视化过程，可以发现由于**超参数**和**随机性**的存在，尽管理论上最优解存在，但SGD找到的结果并不总是最好的。因为我们希望，对于一组数据点，假设数据点到分类边界的距离的集合为$\\{\\gamma_1,\\gamma_2,\\gamma_3,\\cdots, \\gamma_N\\}$，我们所期望的优化效果是\n",
    "$$\n",
    "    \\boldsymbol{w}, b = \\arg\\max_{\\boldsymbol{w}, b}\\min\\{\\gamma_1,\\gamma_2,\\gamma_3,\\cdots, \\gamma_N\\}\n",
    "$$\n",
    "\n",
    "而SVM的优化作为一个凸优化，显然有更好的算法来优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2C4CBB0318C4376850CB0E4424B54A3",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 序列最小优化算法(Sequential Minimal Optimization)与核技巧(Kernel Trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F32CD830A11648A5AB2B3C3A894D1E8C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "SMO算法于1998由John Platt发明，点击可以查看原始论文\n",
    "\n",
    "[Sequential Minimal Optimization:A Fast Algorithm for Training Support Vector Machines](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf)\n",
    "\n",
    "这个算法的效率非常高\n",
    "\n",
    "在课程中，我们已经介绍了我们可以用拉格朗日函数来求解SVM：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})=\\frac{1}{2}||\\boldsymbol{w}||^2 - \\sum_{i=1}^{m}\\alpha_i[y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)}+b) - 1]\n",
    "$$\n",
    "\n",
    "这个函数依然是个凸函数，因此极值点梯度为0，我们对$\\boldsymbol{w}, b$分别求梯度求解得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})}{\\partial \\boldsymbol{w}} = \\boldsymbol{w}-\\sum_{i=1}^{m}\\alpha_i y^{(i)}\\boldsymbol{x}^{(i)}=0 \\Rightarrow \\boldsymbol{w}=\\sum_{i=1}^{m}\\alpha_i y^{(i)}\\boldsymbol{x}^{(i)} \\\\\n",
    "\\frac{\\partial \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})}{\\partial b} = \\sum_{i=1}^{y^{(i)}} \\alpha_i y^{(i)}=0\n",
    "$$\n",
    "\n",
    "等量代换得到：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) & = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B922624CFCED471E9002BE77251F854A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "由于**强对偶**性，我们把问题转化为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) = \\max_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} \\\\\n",
    "= \\min_{\\alpha: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n",
    "\\mathcal{s.t.} \\qquad \\sum_{i=1}^{y^{(i)}} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad \\alpha_i \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "对于**线性不可分**的情况\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n",
    "\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n",
    "$$\n",
    "\n",
    "关于强对偶性以及线性不可分的证明可以参考 **课上内容** 或者 吴恩达的笔记 [Support Vector Machine](http://cs229.stanford.edu/notes/cs229-notes3.pdf)(也可以去网上自行寻找吴恩达的笔记，这个老的链接好像打不开了)，这里不再详细说明\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2A52F022E154E548540EA22B425D0EA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "于是我们就可以通过SMO算法来求解SVM优化问题，可能课上讲的不够详细，这里给出更具体的参考\n",
    "\n",
    "[Stanford CS229 2009 SMO](http://cs229.stanford.edu/materials/smo.pdf)\n",
    "\n",
    "也可以把原始论文作为参考，下面请实现SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "B457201D23734F01A9D8F4882AD5BD74",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference: https://cs229.stanford.edu/materials/smo.pdf\n",
    "\n",
    "def default_kernel(x, z):\n",
    "    return x.dot(z.T)\n",
    "\n",
    "# A faster implementation of the linear error\n",
    "def linear_error(i, X_in, y_in, alpha, b):\n",
    "    E = b - y_in[i]\n",
    "    x = X_in[i]\n",
    "    return E + alpha.dot(y_in * default_kernel(X_in, x))\n",
    "\n",
    "def calculate_error(i, X_in, y_in, alpha, b, kernel):\n",
    "    if (kernel == default_kernel):\n",
    "        return linear_error(i, X_in, y_in, alpha, b)\n",
    "    E = b - y_in[i]\n",
    "    x = X_in[i]\n",
    "    for j in range(len(y_in)):\n",
    "        E += alpha[j] * y_in[j] * kernel(X_in[j], x)\n",
    "    return E\n",
    "\n",
    "def svm_smo(X_in, y_in, C, max_iter, epsilon=1e-5, kernel=default_kernel, max_count = 1919810):\n",
    "    \"\"\"\n",
    "    Inputs have dimension D, we operate on minibatches of N examples.\n",
    "    - X_in: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y_in: A numpy array of shape (N,) containing training labels; y[i] = -1/1 means that X[i] has label -1/1\n",
    "    - kernel: kernel function, using inner product by default\n",
    "    - C: relax factor\n",
    "    - max_iter: max iteration\n",
    "    - epsilon: numerical tolerance\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - alpha: Described as above, a np array of shape (N, )\n",
    "    - b: the bias\n",
    "    \"\"\"\n",
    "\n",
    "    n, _ = X_in.shape\n",
    "    alpha = np.zeros((n,))\n",
    "    b = 0\n",
    "    \n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement SMO Algorithm                                                   #\n",
    "    #############################################################################\n",
    "    \n",
    "    iter    = 0\n",
    "    sum_iter = 0\n",
    "    count   = 0\n",
    "    while iter < max_iter:\n",
    "        num_changed_alphas = 0\n",
    "        for i in range(n):\n",
    "            Ei = calculate_error(i, X_in, y_in, alpha, b, kernel)\n",
    "            if not ((y_in[i] * Ei < -epsilon and alpha[i] < C) or (y_in[i] * Ei > epsilon and alpha[i] > 0)):\n",
    "                continue\n",
    "\n",
    "            j = np.random.choice(np.delete(np.arange(n), i))\n",
    "            Ej = calculate_error(j, X_in, y_in, alpha, b, kernel)\n",
    "\n",
    "            alpha_i_old = alpha[i]\n",
    "            alpha_j_old = alpha[j]\n",
    "\n",
    "            if y_in[i] != y_in[j]:\n",
    "                L = max(0, alpha[j] - alpha[i])\n",
    "                H = min(C, C + alpha[j] - alpha[i])\n",
    "            else:\n",
    "                L = max(0, alpha[j] + alpha[i] - C)\n",
    "                H = min(C, alpha[j] + alpha[i])\n",
    "            if L == H:\n",
    "                continue\n",
    "\n",
    "            Kij = kernel(X_in[i], X_in[j])\n",
    "            Kii = kernel(X_in[i], X_in[i])\n",
    "            Kjj = kernel(X_in[j], X_in[j])\n",
    "\n",
    "            eta = 2 * Kij - Kii - Kjj\n",
    "            if eta >= 0:\n",
    "                continue\n",
    "\n",
    "            alpha[j] = alpha[j] - y_in[j] * (Ei - Ej) / eta\n",
    "            if alpha[j] > H:\n",
    "                alpha[j] = H\n",
    "            elif alpha[j] < L:\n",
    "                alpha[j] = L\n",
    "\n",
    "            if abs(alpha[j] - alpha_j_old) < 1e-5:\n",
    "                continue\n",
    "\n",
    "            alpha[i] = alpha[i] + y_in[i] * y_in[j] * (alpha_j_old - alpha[j])\n",
    "\n",
    "            tmpi = y_in[i] * (alpha[i] - alpha_i_old)\n",
    "            tmpj = y_in[j] * (alpha[j] - alpha_j_old)\n",
    "            b1 = b - Ei - tmpi * Kii - tmpj * Kij\n",
    "            b2 = b - Ej - tmpi * Kij - tmpj * Kjj\n",
    "\n",
    "            if 0 < alpha[i] < C:\n",
    "                b = b1\n",
    "            elif 0 < alpha[j] < C:\n",
    "                b = b2\n",
    "            else:\n",
    "                b = (b1 + b2) / 2\n",
    "            num_changed_alphas += 1\n",
    "\n",
    "        if num_changed_alphas == 0:\n",
    "            iter += 1\n",
    "        else:\n",
    "            iter = 0\n",
    "        sum_iter = max(sum_iter, iter)\n",
    "        count += 1\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print(\"Max iter: \", sum_iter)\n",
    "            print(\"Iteration: \", count)\n",
    "\n",
    "        if count > max_count:\n",
    "            print(\"Max count reached\")\n",
    "            break \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    \n",
    "\n",
    "    return alpha, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F768A0ABD5074DE6A3FC7513EB7F719E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "至此你已经完成了SMO算法，下面的代码会用于测试你的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "529CCCFACD4049DD973C2678F23DC8E6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(x, y, alpha, bias):\n",
    "    category = {'+1': [], '-1': []}\n",
    "    for point, label in zip(x, y):\n",
    "        if label == 1.0: category['+1'].append(point)\n",
    "        else: category['-1'].append(point)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # plot points\n",
    "    for label, pts in category.items():\n",
    "        pts = np.array(pts)\n",
    "        ax.scatter(pts[:, 0], pts[:, 1], label=label)\n",
    "\n",
    "    # calculate weight\n",
    "    weight = 0\n",
    "    for i in range(alpha.shape[0]):\n",
    "        weight += alpha[i] * y[i] * x[i]\n",
    "\n",
    "    # plot the model: wx+b\n",
    "    x1 = np.min(x[:, 0])\n",
    "    y1 = (-bias - weight[0] * x1) / weight[1]\n",
    "    x2 = np.max(x[:, 0])\n",
    "    y2 = (-bias - weight[0] * x2) / weight[1]\n",
    "    ax.plot([x1, x2], [y1, y2])\n",
    "\n",
    "    # plot the support vectors\n",
    "    for i, alpha_i in enumerate(alpha):\n",
    "        if abs(alpha_i) > 1e-3: \n",
    "            ax.scatter([x[i, 0]], [x[i, 1]], s=150, c='none', alpha=0.7,\n",
    "                       linewidth=1.5, edgecolor='#AB3319')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "alpha, bias = svm_smo(x_data, y_data, 1e10, 1000)\n",
    "\n",
    "plot_result(x_data, y_data, alpha, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DCBAE0CEBB24087993A5EF3B565EFA0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "如果一切正常，你会看到一条优秀的分界线，圈出的数据点代表了 **支持向量**，支持向量决定了分界面的位置，直接表现是对应的数据点约束条件的 **拉格朗日乘子系数较大**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B344435F8B34B268A440A7BDF6A81A8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "至此，我们已经对线性SVM以及其优化有了全面的了解，下面我们引入 **核函数** 的概念\n",
    "\n",
    "并不是所有的数据都是平面可以分割的，比如下面的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9E81B0BE926C49B08DD1530CF3FC95E9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('./dataset/Assignment2/data.txt')\n",
    "x_sp = data[:,:2]\n",
    "y_sp = data[:,2]\n",
    "data_visualization(x_sp, y_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D11AB52761B347308D5414C6F966FECB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "显而易见，上面的数据很难用一个平面进行划分，但是我们发现，如果我们将数据点映射到高维空间，事情就会发生变化，但是这种映射难以找到，下面给出一个映射的例子，这个映射不能分割数据，大家可以自行尝试其他的映射方式，此部分不计分，如果能够在三维空间中使得数据线性可分，则可以获得额外分。\n",
    "$$\n",
    "\\phi(\\boldsymbol{x}) = \\begin{bmatrix}\n",
    "x_1^2\\\\\n",
    "x_2^2\\\\\n",
    "x_1 + x_2\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "B164EBBE404846B09D70059C7C04D918",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Input wiht (x, y)\n",
    "# Use (theta, r, r ^ 2) as the new feature\n",
    "# The theta may range from 0 to 2pi\n",
    "\n",
    "x_sp_1 = np.arctan2(x_sp[:, 1], x_sp[:, 0]).reshape(-1, 1)\n",
    "x_sp_2 = np.sqrt(x_sp[:, 0] ** 2 + x_sp[:, 1] ** 2).reshape(-1, 1)\n",
    "\n",
    "x_sp_p = np.concatenate([x_sp_1, x_sp_2, x_sp_2 * x_sp_2],\n",
    "axis=1)\n",
    "# you can try to modify x_sp_p\n",
    "data_visualization(x_sp_p, y_sp, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9883ED2420F9497E801B4CA4BDF5F04D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们已经知道了线性SVM的优化问题：\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n",
    "\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n",
    "$$\n",
    "\n",
    "如果要引入到高维空间的映射，问题变为：\n",
    "\n",
    "$$\n",
    "\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\color{red}{\\phi(\\boldsymbol{x}^{(i)})^{\\top} \\phi(\\boldsymbol{x}^{(j)})} - \\sum_{i=1}^{m} \\alpha_i \\\\\n",
    " =  \\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\color{red}{K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(j)})} - \\sum_{i=1}^{m} \\alpha_i \\\\\n",
    "\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n",
    "$$\n",
    "\n",
    "我们不需要去找到复杂的$\\phi$映射，因为在SVM的优化问题中，我们只需要计算内积，因此，我们只需要找到一个核函数\n",
    "$$\n",
    "K(\\boldsymbol{x}, \\boldsymbol{z}) = \\phi(\\boldsymbol{x})^{\\top}\\phi(\\boldsymbol{z})\n",
    "$$\n",
    "\n",
    "\n",
    "$\\phi$是复杂的，但是核函数是相对容易的，我们在课上给出了四种核函数：\n",
    "\n",
    "- RBF核: $\\exp\\left(-\\frac{\\|\\boldsymbol{x}-\\boldsymbol{z}\\|}{2\\sigma^2}\\right)$\n",
    "\n",
    "- 多项式核: $\\left(\\boldsymbol{x}^\\top\\boldsymbol{z}\\right)^d$\n",
    "\n",
    "- 余弦相似度核: $\\frac{\\boldsymbol{x}^\\top\\boldsymbol{z}}{\\|\\boldsymbol{x}\\|\\cdot\\|\\boldsymbol{z}\\|}$\n",
    "\n",
    "- sigmoid核: $\\tanh(\\alpha\\boldsymbol{x}^\\top\\boldsymbol{z} + c)$\n",
    "\n",
    "现在请实现这些其中常用的RBF,多项式，余弦相似度核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "49EA9797A7B945A6838AC99FC4F0FA91",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def poly_ker_wrap(d):\n",
    "    \"\"\"\n",
    "    polynomial kernel\n",
    "    - d: degree\n",
    "    Returns:\n",
    "    - ker: the kernel function\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement polynomial kernel                                               #\n",
    "    #############################################################################\n",
    "\n",
    "    ker = lambda x, z: (x @ z.T + 1) ** d\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return ker\n",
    "    \n",
    "def cos_ker_wrap():\n",
    "    \"\"\"\n",
    "    cosine similarity kernel\n",
    "    Returns:\n",
    "    - ker: the kernel function\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement cosine similarity kernel                                        #\n",
    "    #############################################################################\n",
    "    \n",
    "    ker = lambda x, z: x @ z.T / (np.linalg.norm(x) * np.linalg.norm(z))\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return ker\n",
    "    \n",
    "def rbf_ker_wrap(sigma):\n",
    "    \"\"\"\n",
    "    RBF kernel\n",
    "    - sigma: variance\n",
    "    Returns:\n",
    "    - ker: the kernel function\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO:                                                                     #\n",
    "    # Implement RBF kernel                                                      #\n",
    "    #############################################################################\n",
    "    \n",
    "    ker = lambda x, z: np.exp(-np.linalg.norm(x - z) ** 2 / (2 * sigma ** 2))\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return ker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22B757C38C634139827FC3CAE67101F1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "RBF核是一个非常常用的核函数，对应的映射函数可以将数据点映射到无线维,对于上面这个线性不可分的数据，RBF核可以起到非常好的效果，运行下面的代码进行比较，你可能需要调整训练轮数来得到比较好的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6AB941BF776C4DC68E9AE4ADC4D85089",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "def plot_boundary(ax, model, x, title):\n",
    "    y = model(x)\n",
    "    y[y < 0], y[y >= 0] = -1, 1\n",
    "\n",
    "    category = {'+1': [], '-1': []}\n",
    "    for point, label in zip(x, y):\n",
    "        if label == 1.0: category['+1'].append(point)\n",
    "        else: category['-1'].append(point)\n",
    "    for label, pts in category.items():\n",
    "        pts = np.array(pts)\n",
    "        ax.scatter(pts[:, 0], pts[:, 1], label=label)\n",
    "\n",
    "    # plot boundary\n",
    "    p = np.meshgrid(np.arange(-1.5, 1.5, 0.025), np.arange(-1.5, 1.5, 0.025))\n",
    "    x = np.array([p[0].flatten(), p[1].flatten()]).T\n",
    "    y = model(x)\n",
    "    y[y < 0], y[y >= 0] = -1, 1\n",
    "    y = np.reshape(y, p[0].shape)\n",
    "    ax.contourf(p[0], p[1], y, cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "def get_model(x_data_used, y_data_used, alpha, b, kernel):\n",
    "    def model(X_in):\n",
    "        results = []\n",
    "        for k in range(X_in.shape[0]):\n",
    "            result = b\n",
    "            for i in range(x_data_used.shape[0]):\n",
    "                result += y_data_used[i] * alpha[i] * kernel(x_data_used[i], X_in[k])\n",
    "            results.append(result)\n",
    "        return np.array(results)\n",
    "    return model\n",
    "\n",
    "\n",
    "# modify the max iteration and numerical tolerance to achieve better performance\n",
    "# alpha, b = svm_smo(x_sp, y_sp, 1e10, 100, 1e-4, default_kernel)\n",
    "# model = get_model(x_sp, y_sp, alpha, b, default_kernel)\n",
    "# plot_boundary(ax1, model, x_sp, 'Default SVM')\n",
    "\n",
    "\n",
    "ker = rbf_ker_wrap(0.2)\n",
    "\n",
    "alpha, b = svm_smo(x_sp, y_sp, 1e10, 800, 1e-5, ker)\n",
    "model = get_model(x_sp, y_sp, alpha, b, ker)\n",
    "plot_boundary(ax2, model, x_sp, 'SVM + RBF')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
