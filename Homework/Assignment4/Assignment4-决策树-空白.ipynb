{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E9863FE77F74AFF814631B9E94C6F5A",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "609a82dc06b94200178edda8",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "***注:Assignement4包含1个代码填空和1个问答题， Assignment5包含2个代码填空和1个问答题**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3F860FDA66141C59656C9EE13D5A2CE",
    "jupyter": {},
    "mdEditEnable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 决策树\n",
    "\n",
    "在这一章节的Jupyter Notebook中，我们将会学习决策树的基本知识并实现ID3决策树模型，并使用实现的模型在Titanic数据集上根据每位乘客的不同特征对乘客进行二分类，用于分析乘客能否在灾难中存活。\n",
    "\n",
    "### 关于决策树\n",
    "\n",
    "决策树是一种常见的机器学习方法，在本章Notebook中，我们将会主要学习基于二分类任务的决策树模型。当基于给定的训练数据集时，决策树对样本的分类可以看做“当前样本属于正类吗？”这个问题进行决策的过程。\n",
    "\n",
    "一般而言，一棵决策树包含一个根节点、若干个内部节点和若干个叶子节点，叶节点对应于决策结果，其他每个节点则对应一个属性测试；每个节点包含的样本集合根据属性测试的结果被划分到子节点中；根节点包含样本全集。决策树学习的目的是为了产生一棵泛化能力强的决策树。\n",
    "\n",
    "当我们在创建一棵决策树时，我们一般遵循以下的递归过程：1）创建根节点，并将所有训练数据都放在根节点中；2）选择一个最优特征，按照这一特征将训练数据集分割成子集；3）如果这些子集已经能够将样本正确分类，那么构建叶节点并将子集放入对应叶节点；4）如果子集尚不能被正确分类，选择新的最优特征并递归以上的操作，直至所有的样本能被正确分类或没有合适的特征为止。\n",
    "\n",
    "### ID3决策树与CART决策树\n",
    "\n",
    "在介绍完决策树创建的基本流程之后，我们会很容易的发现，决策树最关键的地方在于如何选择最优特征，接下来我们将会介绍集中不同的决策树，并分析他们都是如何得到最优特征的。\n",
    "\n",
    "信息熵（Information Entropy）可以反映样本中所包含信息量的平均值，假设样本集$D$中第$k$类样本所占比例为$p_k$，则样本集$D$的信息熵表达式如下所示，\n",
    "$$\n",
    "Entropy(D)=-\\sum_{k=1}^{|y|}p_k\\log_2p_k\n",
    "$$\n",
    "假设离散属性$a$有$V$个可能的取值$\\{a^1,a^2,\\dots,a^V\\}$，若使用$a$来对样本集$D$进行划分，则会产生$V$个分支节点，其中第$v$个分支节点包含了$D$中所有在属性$a$上的取值为$a^V$的样本，记为$D^v$，考虑到不同的分支节点的样本数不同，我们为每个分支赋予权重$|D^v|/|D|$，即样本数越多的分支节点影响越大，最终可以计算出属性$a$对样本集$D$进行划分所获得的”信息增益“。\n",
    "$$\n",
    "Gain\\_Entropy(D,a)=Entropy(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Entropy(D^v)\n",
    "$$\n",
    "著名的ID3决策树就是利用了信息熵增越大，使用属性$a$的划分效果越好的特性，使用信息熵增作为属性选择的准则。\n",
    "\n",
    "与ID3决策树不同，CART决策树使用基尼指数（Gini Index）来选择划分属性，表达式如下：\n",
    "$$\n",
    "Gini(D)=\\sum_{k=1}^{|y|}\\sum_{j\\neq k}p_kp_j = 1-\\sum_{k=1}^{|y|}p_k^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gini\\_Index(D,a)\\sum_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v) \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "基尼指数从样本集$D$中随机抽取两个样本，反映两个样本标签不一致的概率，相比之下，基尼指数不需要对数运算，更加高效，并更加适用于连续属性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question1 试证明用任意的属性a对样本集进行划分，得到的信息增益总是非负的\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "$$\n",
    "Gain\\_Entropy(D,a)=Entropy(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Entropy(D^v)\n",
    "$$\n",
    "\n",
    "单独考虑任意一个事件对信息熵的影响. 不妨设其发生在 $D^v$ 中, 且真实发生的概率为 $p$. 那么, 在原事件中, 其概率 $p'$ 满足 $p' = \\frac{|D^v|}{|D|} p \\le p$\n",
    "\n",
    "因此, 其在原来信息熵中的贡献为 $-p' \\log_2 p' \\ge -p' \\log_2 p$, 而在新的信息熵中的贡献为 $\\frac{|D^v|}{|D|} (-p \\log_2 p) = -p' \\log_2 p$. 我们总是有 $-p' \\log_2 p' \\ge -p' \\log_2 p$. 因此, 每一项的增益都是非负, 故总的信息增益总是非负的.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C86A0BB1B06D490FB58703106649277C",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 数据集\n",
    "\n",
    "在本章节我们将会使用Titanic生存预测数据集，这是一个Kaggle上的公开数据集，对于每位乘客，数据集提供了包含仓位、姓名、性别、年龄等十个特征供我们给数据集进行分类，特征格式包含纯数值、字符串、文本等，下列表格提供了数据集样本的相关信息。\n",
    "\n",
    "| Index | Variable   | Definition                                                   | Type |\n",
    "| ----- | ---------- | ------------------------------------------------------------ | ---- |\n",
    "| 0     | 'Pclass'   | Passenger's class (1st, 2nd, or 3rd)                         | cat  |\n",
    "| 1     | 'Name'     | Passenger's name                                             | text |\n",
    "| 2     | 'Sex'      | Passenger's sex                                              | cat  |\n",
    "| 3     | 'Age'      | Passenger's age                                              | num  |\n",
    "| 4     | 'SibSp'    | Number of siblings/spouses aboard the Titanic                | cat  |\n",
    "| 5     | 'Parch'    | Number of parents/children aboard the Titanic                | cat  |\n",
    "| 6     | 'Ticket'   | Ticket number                                                | text |\n",
    "| 7     | 'Fare'     | Fare paid for ticket                                         | num  |\n",
    "| 8     | 'Cabin'    | Cabin number                                                 | set  |\n",
    "| 9     | 'Embarked' | Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown) | cat  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DD9A7AB629ED4CC89D8FB81DD47E8CFC",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F3B1BD83E05458FBA4CC7A8FE2B9AE2",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 数据集预处理\n",
    "\n",
    "数据集预处理部分，我们将会将特征分为文本特征和数值特征，首先我们将读入文件并按行处理数据集输入，按引号位置将行切分为特征值并储存成list形式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3853370EE3E840A98AFDD45A005F3028",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CAT_FEAT = [0, 2, 4, 5, 9]\n",
    "NUM_FEAT = [3, 7]\n",
    "\n",
    "# 按行处理输入样本，并根据引号将行划分为特征值，存储为list\n",
    "def parse_feat(line):\n",
    "    quota = False\n",
    "    j = 0\n",
    "    feats = []\n",
    "    for i in range(len(line)):\n",
    "        if line[i] == '\\\"':\n",
    "            quota = not quota\n",
    "        if line[i] == ',' and not quota:\n",
    "            feat = line[j:i]\n",
    "            feats.append(feat)\n",
    "            j = i+1\n",
    "    return feats + [line[j:]]\n",
    "\n",
    "\n",
    "# 载入数据集的CSV文件并用刚刚实现的处理函数处理数据\n",
    "def load_file(file_name):\n",
    "    data = []\n",
    "    with open(file_name, 'r') as fin:\n",
    "        print('field_names:', fin.readline().strip().split(','))\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            data.append(parse_feat(line))\n",
    "    return np.array(data)\n",
    "\n",
    "train_data = load_file('./dataset/Assignment4/titanic/train.csv')\n",
    "test_data = load_file('./dataset/Assignment4/titanic/test.csv')\n",
    "\n",
    "train_id, train_label, train_feat = train_data[:, 0], train_data[:, 1], train_data[:, 2:]\n",
    "test_id, test_feat = test_data[:, 0], test_data[:, 1:]\n",
    "\n",
    "# 将数据集输出以便于了解数据集格式\n",
    "print('训练样本特征:\\n', train_feat[0])\n",
    "print('测试样本特征:\\n', test_feat[0])\n",
    "\n",
    "# 将文本特征的特征值设置为None\n",
    "train_feat[:, [1, 6, 8]] = None\n",
    "test_feat[:, [1, 6, 8]] = None\n",
    "\n",
    "print('训练样本特征:\\n', train_feat[0])\n",
    "print('测试样本特征:\\n', test_feat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CCE34E3523C4E7CAE2AA73373D057B4",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num2cat(num_feat, n_class=10):\n",
    "\t# 将数值转换为浮点数的内建函数\n",
    "    def to_float(x):\n",
    "        if len(x):\n",
    "            return float(x)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    num_feat = np.array([to_float(x) for x in num_feat])\n",
    "    min_val, max_val = num_feat[num_feat > -1].min(), num_feat.max()\n",
    "    # 使用linspace函数将函数取值分割为n_class个类别，这里我们默认将n_class设置为10\n",
    "    sep = np.linspace(min_val, max_val, n_class, endpoint=False)\n",
    "    print(sep)\n",
    "    plt.hist(num_feat[num_feat > -1], bins=n_class) \n",
    "    plt.show()\n",
    "\n",
    "    def indicator(x):\n",
    "        x = to_float(x)\n",
    "        if x == -1:\n",
    "            return 0\n",
    "        for i in range(len(sep)):\n",
    "            if x < sep[i]:\n",
    "                return i    \n",
    "        return n_class\n",
    "\n",
    "    return indicator\n",
    "\n",
    "# 使用上述实现的函数完成数据离散化的过程，并可视化数据集离散化之后样本点的分布情况\n",
    "for nf in NUM_FEAT:\n",
    "    ind = num2cat(list(train_feat[:, nf]) + list(test_feat[:, nf]))\n",
    "    for _ in range(len(train_feat[:, nf])):\n",
    "        train_feat[_, nf] = str(ind(train_feat[_, nf]))\n",
    "    for _ in range(len(test_feat[:, nf])):\n",
    "        test_feat[_, nf] = str(ind(test_feat[_, nf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "862C2AFCE4C24C94AD557871C3F2A1FF",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_feat = np.delete(train_feat, [1, 6, 8], axis=1)\n",
    "test_feat = np.delete(test_feat, [1, 6, 8], axis=1)\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('test_feat:\\n', test_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFDE20A05D0B46FF8FF42304663404B1",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 定义Dataset类\n",
    "\n",
    "在这里我们将定义Dataset类，并定义内建函数来更好的处理数据集，我们将会把各个特征转换为one-hot编码的格式以方便之后的计算，我们还会将样本集中20%的数据作为测试集，其余的用于训练集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4E8A7EB5D4384B6EAB84DBBDA842F1DB",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    # 将数据的特征转换为one-hot编码\n",
    "    @staticmethod\n",
    "    def build_feat_map(cat_feats):\n",
    "        feat_map = {}\n",
    "        for i in range(cat_feats.shape[1]):\n",
    "            for x in cat_feats[:, i]:\n",
    "                feat_name = str(i) + ':' + x\n",
    "                if feat_name not in feat_map:\n",
    "                    feat_map[feat_name] = len(feat_map)\n",
    "\n",
    "        return feat_map\n",
    "    \n",
    "    def feat2id(self, cat_feats):\n",
    "        feat_ids = []\n",
    "        for i in range(cat_feats.shape[1]):\n",
    "            feat_ids.append([])\n",
    "            for x in cat_feats[:, i]:\n",
    "                feat_name = str(i) + ':' + x\n",
    "                feat_ids[-1].append(self.feat_map[feat_name])\n",
    "        return np.int32(feat_ids).transpose()\n",
    "    \n",
    "    # 将20%的样本作为测试集，其余的作为训练集\n",
    "    def split_train_valid(self):\n",
    "        np.random.seed(123)\n",
    "        rnd = np.random.random(len(self.train_label))\n",
    "        self.train_ind = np.where(rnd < 0.8)[0]\n",
    "        self.valid_ind = np.where(rnd >= 0.8)[0]\n",
    "\n",
    "        def to_csr(data, dim=len(self.feat_map)):\n",
    "            row = np.zeros_like(data) + np.expand_dims(np.arange(len(data)), 1)\n",
    "            val = np.ones_like(data)\n",
    "            return csr_matrix((val.flatten(), (row.flatten(), data.flatten())), shape=(len(data), dim))    \n",
    "            \n",
    "        self.train_data = (self.train_label[self.train_ind], to_csr(self.train_feat[self.train_ind]))\n",
    "        self.valid_data = (self.train_label[self.valid_ind], to_csr(self.train_feat[self.valid_ind]))\n",
    "        self.test_data = (np.zeros(len(self.test_feat), dtype=np.int32), to_csr(self.test_feat))\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feat_map = self.build_feat_map(np.vstack([train_feat, test_feat]))\n",
    "\n",
    "        self.train_id, self.test_id = train_id, test_id\n",
    "        self.train_label = np.int32(train_label)\n",
    "        self.train_feat, self.test_feat = self.feat2id(train_feat), self.feat2id(test_feat)\n",
    "\n",
    "        print('train_feat:\\n', self.train_feat)\n",
    "        print('test_feat:\\n', self.test_feat)\n",
    "\n",
    "        self.split_train_valid()\n",
    "        \n",
    "Data = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2E415DBA66B24AC6A51AD7039A8B1317",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_label, train_feat = Data.train_data[0], Data.train_data[1].toarray()\n",
    "valid_label, valid_feat = Data.valid_data[0], Data.valid_data[1].toarray()\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('valid_feat:\\n', valid_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25F91693125643438A6835CBC9BB8238",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 实现ID3决策树\n",
    "\n",
    "之前我们已经介绍了决策树的知识内容与ID3决策树、CART决策树，在这里我们将动手实现ID3决策树现在我们来回忆一下ID3决策树的信息增益公式。\n",
    "$$\n",
    "Entropy(D)=-\\sum_{k=1}^{|y|}p_k\\log_2p_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gain\\_Entropy(D,a)=Entropy(D)-\\sum_{v=1}^V\\frac{|D^v|}{|D|}Entropy(D^v)\n",
    "$$\n",
    "\n",
    "在实现完ID3决策树之后，我们将决策树使用在已经准备好的数据集上，并统计决策树预测的准确率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52B35F47949040109699726E5F4BF542",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NID = {}\n",
    "import math\n",
    "class Node:\n",
    "    def __init__(self, feat_id=-1):\n",
    "        self.feat_id = feat_id\n",
    "        self.nid = len(NID)\n",
    "        NID[self.nid] = self\n",
    "        self.t_child = None\n",
    "        self.f_child = None\n",
    "        self._class = -1\n",
    "        \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, n_feat, max_depth=6, verbose=True):\n",
    "        self.n_feat = n_feat\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "        self.root_node = Node()\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(labels):\n",
    "        # All label = 0\n",
    "        sum_0 = (labels == 0).sum()\n",
    "        if sum_0 == 0 or sum_0 == len(labels):\n",
    "            return 0\n",
    "        else:\n",
    "            p0 = float(sum_0) / len(labels)\n",
    "            p1 = 1 - p0\n",
    "            return -p0 * math.log2(p0) - p1 * math.log2(p1)\n",
    "\n",
    "    def fit(self, labels, data, cur_node=None, cur_depth=1):\n",
    "        if cur_node is None:\n",
    "            cur_node = self.root_node\n",
    "\n",
    "        if self.verbose:\n",
    "            print(cur_node.nid)\n",
    "        \n",
    "        if labels.sum() == len(labels):\n",
    "            # 所有instance的label都为1\n",
    "            cur_node._class = 1\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        elif labels.sum() == 0:\n",
    "            # 所有instance的label都为0\n",
    "            cur_node._class = 0\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        elif cur_depth == self.max_depth:\n",
    "            cur_node._class = labels.sum() / len(labels) >= 0.5\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "        \n",
    "        base_ent = self.entropy(labels)\n",
    "        info_gain = 0\n",
    "        best_split = None\n",
    "        best_t_ind = None\n",
    "        best_f_ind = None\n",
    "        \n",
    "        csc_data = data.tocsc()\n",
    "        for f in range(self.n_feat):\n",
    "            feat = csc_data[:, f].toarray().flatten()\n",
    "            t_ind = feat == 1\n",
    "            f_ind = feat == 0\n",
    "            f_ent = base_ent\n",
    "            if t_ind.sum():\n",
    "                f_ent -= t_ind.sum() / len(feat) * self.entropy(labels[t_ind])\n",
    "            if f_ind.sum():\n",
    "                f_ent -= f_ind.sum() / len(feat) * self.entropy(labels[f_ind])\n",
    "            if info_gain < f_ent:\n",
    "                info_gain = f_ent\n",
    "                best_split = f\n",
    "                best_t_ind = t_ind\n",
    "                best_f_ind = f_ind\n",
    "                \n",
    "        if info_gain == 0:\n",
    "            cur_node._class = labels.sum() / len(labels) >= 0.5\n",
    "            cur_node.t_child = None\n",
    "            cur_node.f_child = None\n",
    "            return\n",
    "                \n",
    "        cur_node.feat_id = best_split\n",
    "        cur_node.t_child = Node()\n",
    "        cur_node.f_child = Node()\n",
    "        \n",
    "        self.fit(labels[best_t_ind], data[best_t_ind], cur_node.t_child, cur_depth+1)\n",
    "        self.fit(labels[best_f_ind], data[best_f_ind], cur_node.f_child, cur_depth+1)\n",
    "\n",
    "    def predict(self, data):\n",
    "        assert data.ndim == 1\n",
    "        cur_node = self.root_node\n",
    "        feat_set = set(data)\n",
    "\n",
    "        while True:\n",
    "            if cur_node.t_child is None or cur_node.f_child is None:\n",
    "                return cur_node._class\n",
    "            if cur_node.feat_id in feat_set:\n",
    "                cur_node = cur_node.t_child\n",
    "            else:\n",
    "                cur_node = cur_node.f_child\n",
    "\n",
    "    def batch_predict(self, data):\n",
    "        preds = []\n",
    "        for i in range(data.shape[0]):\n",
    "            preds.append(self.predict(data[i].tocoo().col))\n",
    "        return np.array(preds)\n",
    "\n",
    "    def acc(self, labels, data):\n",
    "        preds = self.batch_predict(data)\n",
    "        acc = np.int32(labels == preds).sum() / len(labels)\n",
    "        return acc\n",
    "\n",
    "DT = DecisionTree(len(Data.feat_map), max_depth=10, verbose=False)   #超参调节max_depth\n",
    "DT.fit(*Data.train_data)\n",
    "\n",
    "print('train acc:',DT.acc(*Data.train_data))\n",
    "print('valid acc:',DT.acc(*Data.valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CDD65DABF4D44CFA69A1AA59BD402D6",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 使用sklearn实现决策树\n",
    "\n",
    "在这一部分中，我们将使用sklearn中封装好的DecisionTreeClassifier来实现决策树，并在Titanic数据集上进行拟合。可以发现最终sklearn的结果会稍微优于我们实现的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "163F411349884A0C9A8A11F18E527D82",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "train_label, train_feat = Data.train_data[0], Data.train_data[1].toarray()\n",
    "valid_label, valid_feat = Data.valid_data[0], Data.valid_data[1].toarray()\n",
    "\n",
    "print('train_feat:\\n', train_feat)\n",
    "print('valid_feat:\\n', valid_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FDD1CDCC5B04E5E80191AF0326E1BB8",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=10)\n",
    "clf.fit(train_feat, train_label)\n",
    "\n",
    "train_preds = clf.predict(train_feat)\n",
    "valid_preds = clf.predict(valid_feat)\n",
    "print('train acc:', np.sum(train_preds == train_label) / len(train_label))\n",
    "print('valid acc:', np.sum(valid_preds == valid_label) / len(valid_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20300094D4F04F6B8D73E03615934725",
    "jupyter": {},
    "mdEditEnable": false,
    "notebookId": "6069a682a7fbf6001890aecf",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 决策树的可视化\n",
    "\n",
    "最后我们可以将训练得到的决策树做可视化，以便于我们观察决策树创建过程中使用了哪些feature。一份样例图像已上传至小作业/tree_view.png，或者你可以使用下方模块生成。\n",
    "\n",
    "**注意：**下方模块需要使用sklearn0.21或以上的plot_tree支持方可运行，如需使用请自行更新版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9304CE0BF1DD4C2B869A4CEBBBCC0D48",
    "jupyter": {},
    "notebookId": "6069a682a7fbf6001890aecf",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "from sklearn import tree\n",
    "\n",
    "fig =plt.figure(figsize=(15, 10),dpi=300)\n",
    "tree.plot_tree(clf,\n",
    "               feature_names=sorted(sorted(Data.feat_map.keys())),\n",
    "               class_names=['non-survival', 'survival'],\n",
    "               filled=True,\n",
    "               fontsize=2)\n",
    "plt.show()\n",
    "fig.savefig('tree_view.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
